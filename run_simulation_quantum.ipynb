{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.cloud_classification_data import get_scene, get_scene_mask, import_scene_names, unzeropad\n",
    "from lib.quantum_classifiers import TA_hybrid_SVM, confusion_values, accuracy_from_confusion_values\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from lib.circuits import WSWS\n",
    "\n",
    "from pennylane import numpy as qnp\n",
    "import pennylane as qml\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, return_minmax = False):\n",
    "    max = data.max(axis=0)\n",
    "    min = data.min(axis=0)\n",
    "    spread = max-min\n",
    "    new_data = (data-min)/spread\n",
    "    if(return_minmax):\n",
    "        return(new_data, min, max)\n",
    "    else:\n",
    "        return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x_train, x_val, n_pca):\n",
    "    x_train_norm, data_min, data_max = normalize_data(x_train, return_minmax=True)\n",
    "    x_validate_norm = normalize_data(x_val)\n",
    "\n",
    "    pca = PCA(n_components=n_pca)\n",
    "    pca.fit(x_train_norm)\n",
    "    \n",
    "    x_train_pca = pca.transform(x_train_norm)\n",
    "    x_validate_pca = pca.transform(x_validate_norm)\n",
    "\n",
    "    x_train, pca_min, pca_max = normalize_data(x_train_pca, return_minmax=True)\n",
    "    x_validate = normalize_data(x_validate_pca)\n",
    "    return(x_train, x_validate, pca, data_min, data_max, pca_min, pca_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access training and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('../38-Cloud_test')\n",
    "scenes_path = base_path/'test_sceneids_38-Cloud.csv'\n",
    "scene_names = import_scene_names(scenes_path)\n",
    "\n",
    "train_path = 'experiment_tr_data/*train*'\n",
    "train_files = sorted(glob.glob(train_path))\n",
    "validate_files = [s.replace('train', 'validate') for s in train_files]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one file from training set\n",
    "file_id = 0\n",
    "\n",
    "# Choose Principal Components number\n",
    "n_pca = 4\n",
    "\n",
    "# Choose hyperparameter range for the validation grid search\n",
    "C_range = (0.01,150,10)# (min, max, step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_id+1, ' of ', len(train_files))\n",
    "train = pd.read_csv(train_files[file_id]).to_numpy()\n",
    "x_train = train[:,:-1]\n",
    "y_train = train[:,-1]\n",
    "\n",
    "validate = pd.read_csv(validate_files[file_id]).to_numpy()\n",
    "x_validate = validate[:,:-1]\n",
    "y_validate = validate[:,-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, pca, data_min, data_max, pca_min, pca_max = prepare_data(x_train, x_validate,n_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_W_layers = 2\n",
    "thetas = 2 * np.pi * qnp.random.random(size=(num_of_W_layers * x_train.shape[1], 3), requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TA_hybrid_SVM(WSWS, thetas, pca=pca, data_min=data_min, data_max=data_max, pca_min=pca_min, pca_max=pca_max, classifier_name='WSWS')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform validation of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.validate(x_train, y_train, x_validate, y_validate, C_range, info = True, max_optimizer_steps=30, n_initializations=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose scene for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_data, r_max, c_max, patches_ids_array, scene_names = get_scene(base_path, scene_id)\n",
    "gt_path = Path('../38-Cloud_test/Entire_scene_gts')\n",
    "gt_scene = get_scene_mask(gt_path, scene_names, scene_id) # Get scene mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict_scene_sp(scene_data=scene_data, r_max=r_max, c_max=c_max, array_of_patches_indices=patches_ids_array, scene_name=scene_names[scene_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_pr_gt = unzeropad(pred,gt_scene)\n",
    "\n",
    "conf_val = confusion_values(trimmed_pr_gt, gt_scene)\n",
    "#clf.export_scene_prediction_results(confusion_values=conf_val, dataset_name = 'SP_pca' + str(n_pca))\n",
    "print('Finished prediction! Scene: ', scene_names[scene_id])\n",
    "print('Scene accurancy: ', accuracy_from_confusion_values(conf_val=conf_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8f4dd9a8150c7db870e9b968ad12732ff3cb84f2df7f35b4515c2f72f48714f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
